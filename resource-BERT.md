# BERT

B: bidirectional
E: encoder
R: representations
T: transformaers

BERT: 
- BERT: Bidirectioanl encoder representations from transformers
- A new method of pre-training language representations which obatains great results on NLP tasks.
- BERT is Google's October 2019 update to the algorithm
([google improvements examples](https://blog.google/products/search/search-language-understanding-bert))

- Can help computers understand language like humans

Why important:
- to train language models based on the entire set of words in a sentence/ query rather than traditionally training on the ordered sequence of words
- allows the language model to learn word **context** based on surrounding words rather than just the word precdes or follows it

This is from the webinar "BERT: Googleâ€™s Latest Search Algorithm to Better Understand Natural Language." Watch the full webinar here: https://youtu.be/9zvsnzgHiWM

[ref video](https://www.youtube.com/watch?v=pcDsHoLE-uo)
