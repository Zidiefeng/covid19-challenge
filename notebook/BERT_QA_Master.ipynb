{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.width',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVq-TuylYRDW"
   },
   "source": [
    "## 1. Install "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f9nhy3PzGQ44"
   },
   "source": [
    "This example uses the `transformers` [library](https://github.com/huggingface/transformers/) by huggingface. We'll start by installing the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "aQl0MMrOGIup",
    "outputId": "60e3a287-f696-429e-d65d-492e0eebd031"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-2.10.0-py3-none-any.whl (660 kB)\n",
      "Collecting tokenizers==0.7.0\n",
      "  Using cached tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8 MB)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Processing /home/ubuntu/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2/sacremoses-0.0.43-py3-none-any.whl\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (0.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers) (4.44.1)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2020.5.14-cp36-cp36m-manylinux2010_x86_64.whl (675 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.1)\n",
      "Installing collected packages: tokenizers, regex, sacremoses, sentencepiece, transformers\n",
      "Successfully installed regex-2020.5.14 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ONLrgJK99TQ"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1WThOUtpYvG-"
   },
   "source": [
    "## 2. Load Fine-Tuned BERT-large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AaweLnNXGhTY"
   },
   "source": [
    "For Question Answering we use the `BertForQuestionAnswering` class from the `transformers` library.\n",
    "\n",
    "This class supports fine-tuning, but for this example we will keep things simpler and load a BERT model that has already been fine-tuned for the SQuAD benchmark.\n",
    "\n",
    "The `transformers` library has a large collection of pre-trained models which you can reference by name and load easily. The full list is in their documentation [here](https://huggingface.co/transformers/pretrained_models.html).\n",
    "\n",
    "For Question Answering, they have a version of BERT-large that has already been fine-tuned for the SQuAD benchmark. \n",
    "\n",
    "BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance. \n",
    "\n",
    "(Note that this download is not using your own network bandwidth--it's between the Google instance and wherever the model is stored on the web).\n",
    "\n",
    "Note: I believe this model was trained on version 1 of SQuAD, since it's not outputting whether the question is \"impossible\" to answer from the text (which is part of the task in v2 of SQuAD).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161,
     "referenced_widgets": [
      "549ca681c27743aa8c28af40d2185900",
      "0c5547ffbd4143609051b231a14853e1",
      "8665ab6d11084a6e980121c5475d8170",
      "461b9936e0864bc1aac73925b730ef7f",
      "49263ffffaee42438fc699c57fab7814",
      "1e6e46c1520d4a309d77a4ff7cc07900",
      "bd8a1fa1079448f585e87dafcd856bfc",
      "2b6be0f1b89044c3a934f3eccf29cf54",
      "6f9df97c25e84e7d8efccce6cde22fbc",
      "f3d792809e234442b6c888b56421479d",
      "94daf90c961945cdafc12178c3e357e4",
      "82461e6ee22b44918d424f24245bdf0e",
      "0e044677b4fc4ffe809b318e8963e08a",
      "930457a02cbe469181b0dbaf1eb9fac0",
      "e86b7da45ef6466ebf25f28c10df61b8",
      "8892ad9564d84ff190062d0a9d5d9f6f"
     ]
    },
    "colab_type": "code",
    "id": "-Mnv95sX-U9K",
    "outputId": "6df46a1c-394d-4439-91e6-9540e9cd2864"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8imoOxoqGZ0h"
   },
   "source": [
    "Load the tokenizer as well. \n",
    "\n",
    "Side note: Apparently the vocabulary of this model is identicaly to the one in bert-base-uncased. You can load the tokenizer from `bert-base-uncased` and that works just as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "632ff7bd64cf4b2c92acee6f0f17d6db",
      "87719f2515a34ba49761e7f18772a90d",
      "f6590ea2e9c14d3099b9999ce1a4602a",
      "b3e5f72146e0406d9977465e9549419f",
      "bf4308a7859f4bfd9356215bbd9ef5e4",
      "d0989443c5e84d6b8eeee08879c07db6",
      "0619db0363894129880d090367b4194a",
      "c17dfd9c89694880a04d07942db3e426"
     ]
    },
    "colab_type": "code",
    "id": "SFQ5f7gv-RBH",
    "outputId": "7e0dd1cf-b565-4c84-b377-2ebee083a9d9"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_pdf_df = pd.read_csv(\"../data/clean_doc_pdf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59561, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_pdf_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the structures are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text</th>\n",
       "      <th>bibliography</th>\n",
       "      <th>raw_authors</th>\n",
       "      <th>raw_bibliography</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001418189999fea7f7cbe3e82703d71c85a6fe5</td>\n",
       "      <td>Absence of surface expression of feline infect...</td>\n",
       "      <td>E Cornelissen, H L Dewerchin, E Van Hamme, H J...</td>\n",
       "      <td>E Cornelissen (Ghent University, Salisburylaan...</td>\n",
       "      <td>Abstract\\n\\nFeline infectious peritonitis viru...</td>\n",
       "      <td>Introduction\\n\\nFeline infectious peritonitis ...</td>\n",
       "      <td>Using direct immunofluorescence to detect coro...</td>\n",
       "      <td>[{'first': 'E', 'middle': [], 'last': 'Corneli...</td>\n",
       "      <td>{'BIBREF0': {'ref_id': 'b0', 'title': 'Using d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0003793cf9e709bc2b9d0c8111186f78fb73fc04</td>\n",
       "      <td>Title: Rethinking high-risk groups in COVID-19</td>\n",
       "      <td>Anastasia Vishnevetsky, Michael Levy</td>\n",
       "      <td>Anastasia Vishnevetsky (Massachusetts General ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\nHow do we protect our 'high-risk' patient ...</td>\n",
       "      <td>COVID-19), , Centers for Disease Control and P...</td>\n",
       "      <td>[{'first': 'Anastasia', 'middle': [], 'last': ...</td>\n",
       "      <td>{'BIBREF0': {'ref_id': 'b0', 'title': 'COVID-1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id  \\\n",
       "0  0001418189999fea7f7cbe3e82703d71c85a6fe5   \n",
       "1  0003793cf9e709bc2b9d0c8111186f78fb73fc04   \n",
       "\n",
       "                                               title  \\\n",
       "0  Absence of surface expression of feline infect...   \n",
       "1     Title: Rethinking high-risk groups in COVID-19   \n",
       "\n",
       "                                             authors  \\\n",
       "0  E Cornelissen, H L Dewerchin, E Van Hamme, H J...   \n",
       "1               Anastasia Vishnevetsky, Michael Levy   \n",
       "\n",
       "                                        affiliations  \\\n",
       "0  E Cornelissen (Ghent University, Salisburylaan...   \n",
       "1  Anastasia Vishnevetsky (Massachusetts General ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Abstract\\n\\nFeline infectious peritonitis viru...   \n",
       "1                                                NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  Introduction\\n\\nFeline infectious peritonitis ...   \n",
       "1  \\n\\nHow do we protect our 'high-risk' patient ...   \n",
       "\n",
       "                                        bibliography  \\\n",
       "0  Using direct immunofluorescence to detect coro...   \n",
       "1  COVID-19), , Centers for Disease Control and P...   \n",
       "\n",
       "                                         raw_authors  \\\n",
       "0  [{'first': 'E', 'middle': [], 'last': 'Corneli...   \n",
       "1  [{'first': 'Anastasia', 'middle': [], 'last': ...   \n",
       "\n",
       "                                    raw_bibliography  \n",
       "0  {'BIBREF0': {'ref_id': 'b0', 'title': 'Using d...  \n",
       "1  {'BIBREF0': {'ref_id': 'b0', 'title': 'COVID-1...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_pdf_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform text:\n",
    "Text column of a dataframe --> dictionarity (keys: paragraph name, values: content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_text_to_dict(df,col_name,output_col,print_it=False):\n",
    "    text_list=[]\n",
    "    for i in df.index:\n",
    "        line = df.loc[i]\n",
    "        text=str(line[col_name])\n",
    "        text_dict=dict()\n",
    "        comp_list= text.split(\"\\n\\n\")\n",
    "        for num in range(int((len(comp_list))/2)):\n",
    "            key_str=str(num)+'_'+str(comp_list[num*2])\n",
    "            key_str=key_str.strip()\n",
    "            text_dict[key_str]=str(comp_list[num*2+1])\n",
    "        text_list.append(text_dict)\n",
    "        if print_it ==True:\n",
    "            print(i)\n",
    "    df[output_col]=text_list\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap_dataframe=df_text_to_dict(clean_pdf_df,\"text\",\"text_dict\",False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>affiliations</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text</th>\n",
       "      <th>bibliography</th>\n",
       "      <th>raw_authors</th>\n",
       "      <th>raw_bibliography</th>\n",
       "      <th>text_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001418189999fea7f7cbe3e82703d71c85a6fe5</td>\n",
       "      <td>Absence of surface expression of feline infect...</td>\n",
       "      <td>E Cornelissen, H L Dewerchin, E Van Hamme, H J...</td>\n",
       "      <td>E Cornelissen (Ghent University, Salisburylaan...</td>\n",
       "      <td>Abstract\\n\\nFeline infectious peritonitis viru...</td>\n",
       "      <td>Introduction\\n\\nFeline infectious peritonitis ...</td>\n",
       "      <td>Using direct immunofluorescence to detect coro...</td>\n",
       "      <td>[{'first': 'E', 'middle': [], 'last': 'Corneli...</td>\n",
       "      <td>{'BIBREF0': {'ref_id': 'b0', 'title': 'Using d...</td>\n",
       "      <td>{'0_Introduction': 'Feline infectious peritoni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0003793cf9e709bc2b9d0c8111186f78fb73fc04</td>\n",
       "      <td>Title: Rethinking high-risk groups in COVID-19</td>\n",
       "      <td>Anastasia Vishnevetsky, Michael Levy</td>\n",
       "      <td>Anastasia Vishnevetsky (Massachusetts General ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\n\\nHow do we protect our 'high-risk' patient ...</td>\n",
       "      <td>COVID-19), , Centers for Disease Control and P...</td>\n",
       "      <td>[{'first': 'Anastasia', 'middle': [], 'last': ...</td>\n",
       "      <td>{'BIBREF0': {'ref_id': 'b0', 'title': 'COVID-1...</td>\n",
       "      <td>{'0_': 'How do we protect our 'high-risk' pati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id  \\\n",
       "0  0001418189999fea7f7cbe3e82703d71c85a6fe5   \n",
       "1  0003793cf9e709bc2b9d0c8111186f78fb73fc04   \n",
       "\n",
       "                                               title  \\\n",
       "0  Absence of surface expression of feline infect...   \n",
       "1     Title: Rethinking high-risk groups in COVID-19   \n",
       "\n",
       "                                             authors  \\\n",
       "0  E Cornelissen, H L Dewerchin, E Van Hamme, H J...   \n",
       "1               Anastasia Vishnevetsky, Michael Levy   \n",
       "\n",
       "                                        affiliations  \\\n",
       "0  E Cornelissen (Ghent University, Salisburylaan...   \n",
       "1  Anastasia Vishnevetsky (Massachusetts General ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Abstract\\n\\nFeline infectious peritonitis viru...   \n",
       "1                                                NaN   \n",
       "\n",
       "                                                text  \\\n",
       "0  Introduction\\n\\nFeline infectious peritonitis ...   \n",
       "1  \\n\\nHow do we protect our 'high-risk' patient ...   \n",
       "\n",
       "                                        bibliography  \\\n",
       "0  Using direct immunofluorescence to detect coro...   \n",
       "1  COVID-19), , Centers for Disease Control and P...   \n",
       "\n",
       "                                         raw_authors  \\\n",
       "0  [{'first': 'E', 'middle': [], 'last': 'Corneli...   \n",
       "1  [{'first': 'Anastasia', 'middle': [], 'last': ...   \n",
       "\n",
       "                                    raw_bibliography  \\\n",
       "0  {'BIBREF0': {'ref_id': 'b0', 'title': 'Using d...   \n",
       "1  {'BIBREF0': {'ref_id': 'b0', 'title': 'COVID-1...   \n",
       "\n",
       "                                           text_dict  \n",
       "0  {'0_Introduction': 'Feline infectious peritoni...  \n",
       "1  {'0_': 'How do we protect our 'high-risk' pati...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap_dataframe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_ap_dataframe=ap_dataframe[[\"paper_id\",\"text_dict\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0_'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_ap_dataframe.loc[1][\"text_dict\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59561, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_ap_dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create `appended dataframe` with `text_dict` and `paper_id` only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_df=sm_ap_dataframe[[\"paper_id\",\"text_dict\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>text_dict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001418189999fea7f7cbe3e82703d71c85a6fe5</td>\n",
       "      <td>{'0_Introduction': 'Feline infectious peritoni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0003793cf9e709bc2b9d0c8111186f78fb73fc04</td>\n",
       "      <td>{'0_': 'How do we protect our 'high-risk' pati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00039b94e6cb7609ecbddee1755314bcfeb77faa</td>\n",
       "      <td>{'0_INTRODUCTION': 'Severe acute respiratory s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0004456994f6c1d5db7327990386d33c01cff32a</td>\n",
       "      <td>{'0_Background': 'The German standing commissi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004774b55eb0dad880aba9b572efe362660c5e0</td>\n",
       "      <td>{'0_': '. So, if there is no singular definiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59556</th>\n",
       "      <td>fff69e4894df7b4134bb2ddc830764459ac3edbe</td>\n",
       "      <td>{'0_INTRODUCTION': 'Coronavirus (CoV) infectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59557</th>\n",
       "      <td>fff6fe12beb51ee2641ddb5381378ff3560d8103</td>\n",
       "      <td>{'0_Introduction': 'Worldwide, the adoption of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59558</th>\n",
       "      <td>fff6febdd287d474d7950b14faa899c4095557b3</td>\n",
       "      <td>{'0_': 'pneumonia had not been the \"friend of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59559</th>\n",
       "      <td>fff8b9e88db122ffcbaf1daf6b697e44eaaffd93</td>\n",
       "      <td>{'0_': 'Sir: Septic shock due to Mycobacterium...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59560</th>\n",
       "      <td>fffaed7e9353b7df6c4ca8f66b62e117013cb86d</td>\n",
       "      <td>{'0_': 'Most DENV infections are asymptomatic ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59561 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       paper_id  \\\n",
       "0      0001418189999fea7f7cbe3e82703d71c85a6fe5   \n",
       "1      0003793cf9e709bc2b9d0c8111186f78fb73fc04   \n",
       "2      00039b94e6cb7609ecbddee1755314bcfeb77faa   \n",
       "3      0004456994f6c1d5db7327990386d33c01cff32a   \n",
       "4      0004774b55eb0dad880aba9b572efe362660c5e0   \n",
       "...                                         ...   \n",
       "59556  fff69e4894df7b4134bb2ddc830764459ac3edbe   \n",
       "59557  fff6fe12beb51ee2641ddb5381378ff3560d8103   \n",
       "59558  fff6febdd287d474d7950b14faa899c4095557b3   \n",
       "59559  fff8b9e88db122ffcbaf1daf6b697e44eaaffd93   \n",
       "59560  fffaed7e9353b7df6c4ca8f66b62e117013cb86d   \n",
       "\n",
       "                                               text_dict  \n",
       "0      {'0_Introduction': 'Feline infectious peritoni...  \n",
       "1      {'0_': 'How do we protect our 'high-risk' pati...  \n",
       "2      {'0_INTRODUCTION': 'Severe acute respiratory s...  \n",
       "3      {'0_Background': 'The German standing commissi...  \n",
       "4      {'0_': '. So, if there is no singular definiti...  \n",
       "...                                                  ...  \n",
       "59556  {'0_INTRODUCTION': 'Coronavirus (CoV) infectio...  \n",
       "59557  {'0_Introduction': 'Worldwide, the adoption of...  \n",
       "59558  {'0_': 'pneumonia had not been the \"friend of ...  \n",
       "59559  {'0_': 'Sir: Septic shock due to Mycobacterium...  \n",
       "59560  {'0_': 'Most DENV infections are asymptomatic ...  \n",
       "\n",
       "[59561 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test sm sm df ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_sm_df=sm_df.loc[0:10].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport time    \\n\\nstart = start = time.time()\\nwith open(\"test.txt\", \\'w\\') as f:\\n    for i in range(10000000):\\n        # print(\\'This is a speed test\\', file=f)\\n        # f.write(\\'This is a speed test\\n\\')\\nend = time.time()\\nprint(end - start)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import time    \n",
    "\n",
    "start = start = time.time()\n",
    "with open(\"test.txt\", 'w') as f:\n",
    "    for i in range(10000000):\n",
    "        # print('This is a speed test', file=f)\n",
    "        # f.write('This is a speed test\\n')\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paper_id': '00060fb61742ff60e4e3ba4648c74a34cfe9560d',\n",
       " 'text_dict': {'0_Introduction': 'The outbreak of a novel coronavirus disease has led to the current global health crisis. As of late March, there have been over 315,000 cases worldwide and over 25,000 cases in the United States [1] . COVID-19 has been of concern to many health care workers (HCW\\'s) in the emergency department (ED), due to potential exposure and the possibility of transmitting the virus to other patients and family members. Currently, personal protection equipment (PPE) recommendations are in flux and it is unclear if patients outside of \"flu-like symptoms\" require PPE [2] . Furthermore, as increasing asymptomatic and atypical presentations develop, clear guidance is needed for HCW\\'s. In China, asymptomatic and atypical presentations with gastrointestinal symptoms have been reported [3] , and it has been suggested that up to 10% of COVID-19 patients present with gastrointestinal symptoms. However, due to limited data and an unknown true disease prevalence, it is unclear what percentage of patients present atypically without respiratory symptoms [4] . In addition, a study showed that up to 18% of patients had no radiographic or CT abnormality [5] . As studies continue to demonstrate more patients who present asymptomatically or atypically, our need for increased guidance on PPE grows. We present a case report of a man who was referred to the ED for abdominal and testicular pain who was subsequently found to test positive for COVID-19.',\n",
       "  '1_Case report': \"A 42 year-old-male was referred by his primary care physician (PCP) to the ED for further evaluation of 8 days of abdominal, testicular, and back pain. The patient originally presented to his PCP's office one week prior, where he was diagnosed with constipation and sent home on a bowel regimen. The pain intermittently continued with waxing and waning severity for 8 days prompting his ED presentation. The patient described his symptoms as a constant stabbing pain that originated from his groin and migrated to his abdomen, flank, back, and chest. Additionally, he reported subjective fevers two days before presentation. On review of systems, he denied any rhinorrhea, sore throat, cough, shortness of breath, nausea or vomiting.On arrival to the ED, the patient was afebrile and hemodynamically stable. The physical exam revealed diffuse abdominal tenderness without guarding or rebound and the testicular exam revealed no abnormal findings. The remainder of the patient's physical exam was otherwise normal. Standard blood work, a chest radiograph (Fig. 1) , and abdominal CT scan (Figs. 2 and 3) were obtained. The patient's blood laboratory values were benign including a normal white blood count. His chest radiograph revealed no abnormal findings as read by radiology. The CT of his abdomen and pelvis captured the lung bases which revealed pulmonary ground glass opacification and consolidation consistent with pneumonia in addition to possible colitis of the sigmoid and distal descending colon. The patient was subsequently started on cefpodoxime and azithromycin for treatment of both his colitis and pneumonia and discharged home with instruction to follow up with his PCP.Two days later, the patient's PCP telephoned the primary ED team to notify the team that the patient had tested positive for COVID-19. The team was notified that one day prior to his ED visit, the patient had received COVID-19 testing due to his attendance at a biotechnology conference two weeks prior that was known to be the nidus of the Massachusetts outbreak. On the day of his ED visit, the patient had been seated in the waiting room for 2 h prior to being roomed. Unfortunately, full PPE was never donned prior to his examination.\",\n",
       "  '2_Discussion': \"Asymptomatic and atypical presentations of COVID-19 will continue to present to the ED as the number of COVID-19 cases rise. The patient described in this case report was not demonstrating any respiratory symptoms nor was the team aware that the patient was recently tested prior to his ED visit therefore, proper PPE was not worn by the treating HCW's. As a result, over 25 patients and HCW's were notified of their potential exposure.The patient's radiological findings were enough to cause us to pause in this case. As reported, the sensitivity of CT scans is 97% with as compared to the gold standard of a positive RT-PCR result American Journal of Emergency Medicine xxx (xxxx) xxx [6] . A combination of GGO and consolidative opacities is the most common radiologic finding on chest CT and can be found in up to 88% of cases with COVID-19 [5] [6] [7] [8] . In addition, multilobar and posterior involvements have been observed [8] . Unfortunately, given the date of presentation, atypical features and lack of communication we were not able to piece the findings to the patient's symptoms at that time.The atypical presentation and lack of communication in this case led to unnecessary HCW's exposures and risk. As community spread becomes more prevalent and PPE shortages continue, it is up to the health care team to assess the level of risk [2, 8] . However, it is the authors' opinion that given the of risk of COVID-19 to HCW's and continued atypical presentations that PPE to include a face mask should be considered for every patient depending on the incidence of COVID-19 within their community. As we continue in this health crisis it is critical that communication continues to be of the utmost importance [9] . In this case, communication could have protected dozens of patients and HCW's. It should be noted that this case was at the beginning of the United States outbreak when national and institutional standards were in their infancy, and new adaptations to these standards are being made daily. We present this case as a lesson learned from the front lines and to bring awareness of atypical COVID-19 cases as they continue to present.\",\n",
       "  '3_Conclusion': \"Asymptomatic and atypical presentations of COVID-19 have been described, but the frequency is unknown [10] . HCW's need to be vigilant of atypical presentations of COVID-19 and consider full PPE for all patients as community spread increases.\",\n",
       "  '4_Presentations': 'None.',\n",
       "  '5_Financial support': 'None.',\n",
       "  '6_Author contributions': 'JK: drafted the manuscript. NS: edited manuscript and assisted in final draft. AJG: provided mentorship and edited manuscript.',\n",
       "  '7_Declaration of competing interest': 'The authors report no conflict of interest. '}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(json.dumps(sm_sm_df.loc[5].to_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Json Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_sm_sm_json_line.txt\", 'w') as f:\n",
    "    for i in sm_sm_df.index:\n",
    "        json_content=json.dumps(sm_sm_df.loc[i].to_dict())\n",
    "        f.write(json_content)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Json load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "with open(\"test_sm_sm_json_line.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        try: #print(line)\n",
    "            json.loads(line)\n",
    "            print(\"yes\")\n",
    "        except:\n",
    "            #print(line)\n",
    "            print(\"no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----Test_Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Code-Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sm_ap_df_json.txt\", 'w') as f:\n",
    "    for i in sm_sm_df.index:\n",
    "        json_content=json.dumps(sm_df.loc[i].to_dict())\n",
    "        f.write(json_content)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n",
      "yes\n"
     ]
    }
   ],
   "source": [
    "with open(\"test_sm_sm_json_line.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        try: #print(line)\n",
    "            json.loads(line)\n",
    "            print(\"yes\")\n",
    "        except:\n",
    "            #print(line)\n",
    "            print(\"no\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Q_A Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, answer_text):\n",
    "    '''\n",
    "    Takes a `question` string and an `answer_text` string (which contains the\n",
    "    answer), and identifies the words within the `answer_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # ======== Tokenize ========\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    input_ids = tokenizer.encode(question, answer_text,max_length=500\n",
    "                                )\n",
    "\n",
    "    # Report how long the input sequence is.\n",
    "    #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "    # ======== Set Segment IDs ========\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # ======== Evaluate ========\n",
    "    # Run our example question through the model.\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n",
    "\n",
    "    # ======== Reconstruct Answer ========\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "    \n",
    "    \n",
    "    # Get the string versions of the input tokens.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        \n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "            \n",
    "    s_scores = start_scores.detach().numpy().flatten()\n",
    "    e_scores = end_scores.detach().numpy().flatten()\n",
    "    #print('score:'+(start_scores)+\"; \"+str(end_scores))\n",
    "    #print('score:'+str(max(s_scores))+\"; \"+str(min(e_scores)))\n",
    "    #print(str(tensor[torch.argmax(start_scores)]))\n",
    "    #print('Answer: \"' + answer + '\"')\n",
    "    return [answer,str(max(s_scores)),len(input_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor i in clean_pdf_df.text:\\n    answer_question(\"What do we know about Hypertension?\",i)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for i in clean_pdf_df.text:\n",
    "    answer_question(\"What do we know about Hypertension?\",i)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_example=\"What do we know about Hypertension?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Q_example_a=\"[CLS] what do we know about hypertension ? [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sm_ap_dataframe[[\"paper_id\",\"text_dict\"]].to_csv(\"sm_appended_df.csv\",sep='`', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(\"sm_appended_df.csv\",sep='`',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the percentage of dead fipv positive monocytes in pyogranulomas was significantly higher than the control cells\n",
      "0.03644225\n",
      "101\n",
      "plasma cytokine and chemokine concentrations were not in a gaussian distribution\n",
      "0.46187294\n",
      "107\n",
      "plasma pmn cc chemokine il - 8 , monocyte cc chemokine mcp - 1 and th1 cxc chemokine ip - 10 concentrations were elevated\n",
      "0.12026076\n",
      "119\n",
      "our study results suggest that hcws in an acute hospital care setting are at no higher risk of influenza than the general public\n",
      "1.0827159\n",
      "202\n",
      "how first responders and the authorities perceived the event ( s )\n",
      "0.49143612\n",
      "74\n",
      "the frequency is unknown\n",
      "2.6545522\n",
      "72\n",
      "the authors report no conflict of interest\n",
      "1.3563815\n",
      "19\n",
      ".\n",
      "2.3439858\n",
      "450\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-ea730d11853d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0manswer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manswer_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_example\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0ms_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manswer_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_example\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtoken_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manswer_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_example\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-997f94ca03ff>\u001b[0m in \u001b[0;36manswer_question\u001b[0;34m(question, answer_text)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Run our example question through the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n\u001b[0;32m---> 34\u001b[0;31m                                     token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# ======== Reconstruct Answer ========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions)\u001b[0m\n\u001b[1;32m   1444\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1446\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1447\u001b[0m         )\n\u001b[1;32m   1448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_extended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m         )\n\u001b[1;32m    736\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             layer_outputs = layer_module(\n\u001b[0;32m--> 407\u001b[0;31m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             )\n\u001b[1;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     ):\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0mself_attention_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add self attentions if we output attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    312\u001b[0m     ):\n\u001b[1;32m    313\u001b[0m         self_outputs = self.self(\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         )\n\u001b[1;32m    316\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlp/lib/python3.7/site-packages/transformers/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;31m# Take the dot product between \"query\" and \"key\" to get the raw attention scores.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_head_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in ap_dataframe.index:\n",
    "    line=ap_dataframe.loc[i]\n",
    "    dct=line[\"text_dict\"]\n",
    "    for key in dct.keys():\n",
    "        answer=answer_question(Q_example,dct[key])[0]\n",
    "        s_score=answer_question(Q_example,dct[key])[1]\n",
    "        token_len=answer_question(Q_example,dct[key])[2]\n",
    "        if (answer!=\"[CLS]\") & (float(s_score)>0) & (answer!=Q_example_a):\n",
    "            print(answer)\n",
    "            print(s_score)\n",
    "            print(token_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pNRVuaKSNFG8"
   },
   "source": [
    "Just to see exactly what the tokenizer is doing, let's print out the tokens with their IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"What is the best method to combat the hypercoagulable state seen in COVID-19 ?\"\n",
    "#from mrjob.job import MRJob\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "import pandas as pd\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "data_output=[]\n",
    "with open(\"test_sm_sm_json_line.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        con=dict(json.loads(line))\n",
    "        print(type(con))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001418189999fea7f7cbe3e82703d71c85a6fe50_Introduction\n",
      "0001418189999fea7f7cbe3e82703d71c85a6fe51_Cats with naturally occurring FIP\n",
      "0001418189999fea7f7cbe3e82703d71c85a6fe52_Antibodies\n",
      "0001418189999fea7f7cbe3e82703d71c85a6fe53_Isolation of FIPV positive cells\n",
      "0001418189999fea7f7cbe3e82703d71c85a6fe54_Characterization of FIPV positive cells\n",
      "0001418189999fea7f7cbe3e82703d71c85a6fe55_Cultivation of FIPV positive cells\n",
      "0001418189999fea7f7cbe3e82703d71c85a6fe56_Statistics\n",
      "0001418189999fea7f7cbe3e82703d71c85a6fe57_Results\n",
      "0001418189999fea7f7cbe3e82703d71c85a6fe58_lane C). No difference in amount\n",
      "0001418189999fea7f7cbe3e82703d71c85a6fe59_Discussion\n"
     ]
    }
   ],
   "source": [
    "data_output=[]\n",
    "with open(\"test_sm_sm_json_line.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        if len(data_output)>3:\n",
    "            break\n",
    "        con=json.loads(line)\n",
    "        paper_id=con[\"paper_id\"]\n",
    "        text_dict=con[\"text_dict\"]\n",
    "        for key in text_dict.keys():\n",
    "            content = text_dict[key]\n",
    "            com_id=str(paper_id)+str(key)\n",
    "\n",
    "            answer_text=str(content)        \n",
    "\n",
    "            input_ids = tokenizer.encode(question, answer_text,max_length=500)\n",
    "\n",
    "            # Report how long the input sequence is.\n",
    "            #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "            # ======== Set Segment IDs ========\n",
    "            # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "            sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "            # The number of segment A tokens includes the [SEP] token istelf.\n",
    "            num_seg_a = sep_index + 1\n",
    "\n",
    "            # The remainder are segment B.\n",
    "            num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "            # Construct the list of 0s and 1s.\n",
    "            segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "            # There should be a segment_id for every input token.\n",
    "            assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "            # ======== Evaluate ========\n",
    "            # Run our example question through the model.\n",
    "            start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                            token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n",
    "\n",
    "            # ======== Reconstruct Answer ========\n",
    "            # Find the tokens with the highest `start` and `end` scores.\n",
    "            answer_start = torch.argmax(start_scores)\n",
    "            answer_end = torch.argmax(end_scores)\n",
    "\n",
    "\n",
    "            # Get the string versions of the input tokens.\n",
    "            tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "            # Start with the first token.\n",
    "            answer = tokens[answer_start]\n",
    "\n",
    "            # Select the remaining answer tokens and join them with whitespace.\n",
    "            for i in range(answer_start + 1, answer_end + 1):\n",
    "\n",
    "                # If it's a subword token, then recombine it with the previous token.\n",
    "                if tokens[i][0:2] == '##':\n",
    "                    answer += tokens[i][2:]\n",
    "\n",
    "                # Otherwise, add a space then the token.\n",
    "                else:\n",
    "                    answer += ' ' + tokens[i]\n",
    "\n",
    "            s_scores = start_scores.detach().numpy().flatten()\n",
    "            e_scores = end_scores.detach().numpy().flatten()  \n",
    "            print(com_id)\n",
    "            if (answer!=\"[CLS]\") & (float(max(s_scores))>0)& (len(answer)>0):\n",
    "                data_output.append([paper_id,key,answer,max(s_scores)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0001418189999fea7f7cbe3e82703d71c85a6fe5',\n",
       "  '1_Cats with naturally occurring FIP',\n",
       "  'fcov antibody',\n",
       "  2.663851],\n",
       " ['0001418189999fea7f7cbe3e82703d71c85a6fe5',\n",
       "  '5_Cultivation of FIPV positive cells',\n",
       "  'immunofluorescence staining',\n",
       "  3.4416032],\n",
       " ['0001418189999fea7f7cbe3e82703d71c85a6fe5',\n",
       "  '6_Statistics',\n",
       "  'wilcoxon signed ranks test',\n",
       "  1.8611692],\n",
       " ['0001418189999fea7f7cbe3e82703d71c85a6fe5',\n",
       "  '8_lane C). No difference in amount',\n",
       "  'viability staining',\n",
       "  2.6267781]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sm_ap_dataframe[[\"paper_id\",\"text_dict\"]].to_json(\"sm_appended_df.json\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Question Answering with a Fine-Tuned BERT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0619db0363894129880d090367b4194a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0c5547ffbd4143609051b231a14853e1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e044677b4fc4ffe809b318e8963e08a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "1e6e46c1520d4a309d77a4ff7cc07900": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2b6be0f1b89044c3a934f3eccf29cf54": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "461b9936e0864bc1aac73925b730ef7f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2b6be0f1b89044c3a934f3eccf29cf54",
      "placeholder": "​",
      "style": "IPY_MODEL_bd8a1fa1079448f585e87dafcd856bfc",
      "value": "100% 398/398 [00:00&lt;00:00, 11.2kB/s]"
     }
    },
    "49263ffffaee42438fc699c57fab7814": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "549ca681c27743aa8c28af40d2185900": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8665ab6d11084a6e980121c5475d8170",
       "IPY_MODEL_461b9936e0864bc1aac73925b730ef7f"
      ],
      "layout": "IPY_MODEL_0c5547ffbd4143609051b231a14853e1"
     }
    },
    "632ff7bd64cf4b2c92acee6f0f17d6db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f6590ea2e9c14d3099b9999ce1a4602a",
       "IPY_MODEL_b3e5f72146e0406d9977465e9549419f"
      ],
      "layout": "IPY_MODEL_87719f2515a34ba49761e7f18772a90d"
     }
    },
    "6f9df97c25e84e7d8efccce6cde22fbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_94daf90c961945cdafc12178c3e357e4",
       "IPY_MODEL_82461e6ee22b44918d424f24245bdf0e"
      ],
      "layout": "IPY_MODEL_f3d792809e234442b6c888b56421479d"
     }
    },
    "82461e6ee22b44918d424f24245bdf0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8892ad9564d84ff190062d0a9d5d9f6f",
      "placeholder": "​",
      "style": "IPY_MODEL_e86b7da45ef6466ebf25f28c10df61b8",
      "value": "100% 1.34G/1.34G [00:33&lt;00:00, 40.4MB/s]"
     }
    },
    "8665ab6d11084a6e980121c5475d8170": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e6e46c1520d4a309d77a4ff7cc07900",
      "max": 398,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_49263ffffaee42438fc699c57fab7814",
      "value": 398
     }
    },
    "87719f2515a34ba49761e7f18772a90d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8892ad9564d84ff190062d0a9d5d9f6f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "930457a02cbe469181b0dbaf1eb9fac0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94daf90c961945cdafc12178c3e357e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_930457a02cbe469181b0dbaf1eb9fac0",
      "max": 1340675298,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0e044677b4fc4ffe809b318e8963e08a",
      "value": 1340675298
     }
    },
    "b3e5f72146e0406d9977465e9549419f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c17dfd9c89694880a04d07942db3e426",
      "placeholder": "​",
      "style": "IPY_MODEL_0619db0363894129880d090367b4194a",
      "value": "100% 232k/232k [00:00&lt;00:00, 1.18MB/s]"
     }
    },
    "bd8a1fa1079448f585e87dafcd856bfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bf4308a7859f4bfd9356215bbd9ef5e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "c17dfd9c89694880a04d07942db3e426": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d0989443c5e84d6b8eeee08879c07db6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e86b7da45ef6466ebf25f28c10df61b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3d792809e234442b6c888b56421479d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f6590ea2e9c14d3099b9999ce1a4602a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d0989443c5e84d6b8eeee08879c07db6",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bf4308a7859f4bfd9356215bbd9ef5e4",
      "value": 231508
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
