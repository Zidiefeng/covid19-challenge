{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MRJob on EMR (Elastic Map Reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list=[]\n",
    "with open(\"sm_df.txt\",\"r\") as file:\n",
    "    for i in file:\n",
    "        data_list.append(json.dumps(i))\n",
    "        if len(data_list)>4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sm_df_sample.txt\",\"w\") as f:\n",
    "    for line in data_list:\n",
    "        json_content=json.dumps(i)\n",
    "        f.write(json_content)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "data_list=[]\n",
    "with open(\"sm_df_sample.txt\",\"r\") as file:\n",
    "    for i in file:\n",
    "       print(type(json.loads(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mrjob in /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages (0.7.2)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages (from mrjob) (5.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install mrjob\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!pip install mrjob\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create configuration file for mrjob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with 8 core instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .mrjob.conf\n"
     ]
    }
   ],
   "source": [
    "%%file .mrjob.conf\n",
    "\n",
    "# http://mrjob.readthedocs.io/en/stable/guides/emr-opts.html\n",
    "\n",
    "runners:\n",
    "  dataproc:\n",
    "    instance_type: n1-standard-2\n",
    "    #instance_type: e2-standard-8\n",
    "    num_core_instances: 2\n",
    "  local:\n",
    "    #instance_type: e2-standard-8\n",
    "    #num_core_instances: 8\n",
    "  emr:\n",
    "    aws_access_key_id: AKIAI5ELHG5OLN757ENQ\n",
    "    aws_secret_access_key: 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\n",
    "    ec2_key_pair: ka200510\n",
    "    ec2_key_pair_file: /home/ubuntu/covid19-challenge/notebook/ka200510.pem\n",
    "    region: us-west-2 # http://docs.aws.amazon.com/general/latest/gr/rande.html\n",
    "    master_instance_type: m5a.8xlarge # https://aws.amazon.com/emr/pricing/\n",
    "    instance_type: m5a.8xlarge\n",
    "    num_core_instances: 8\n",
    "    max_mins_idle: 20\n",
    "    ssh_tunnel: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .mrjob.conf\n"
     ]
    }
   ],
   "source": [
    "%%file .mrjob.conf\n",
    "\n",
    "# http://mrjob.readthedocs.io/en/stable/guides/emr-opts.html\n",
    "\n",
    "runners:\n",
    "  dataproc:\n",
    "    instance_type: n1-standard-2\n",
    "    #instance_type: e2-standard-8\n",
    "    num_core_instances: 2\n",
    "  local:\n",
    "    #instance_type: e2-standard-8\n",
    "    #num_core_instances: 8\n",
    "  emr:\n",
    "    aws_access_key_id: AKIAI5ELHG5OLN757ENQ\n",
    "    aws_secret_access_key: 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\n",
    "    ec2_key_pair: ka200510\n",
    "    ec2_key_pair_file: /home/ubuntu/covid19-challenge/notebook/ka200510.pem\n",
    "    region: us-west-2 # http://docs.aws.amazon.com/general/latest/gr/rande.html\n",
    "    master_instance_type: m5a.8xlarge # https://aws.amazon.com/emr/pricing/\n",
    "    instance_type: c5.2xlarge\n",
    "    num_core_instances: 0\n",
    "    \n",
    "    ssh_tunnel: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export GOOGLE_APPLICATION_CREDENTIALS=\"/home/jupyter/covid19-challenge/notebook mindful-carport-278217-556456672726.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create program: MRJob .py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: \n",
    "- Q1: `What is the best method to combat the hypercoagulable state seen in COVID-19`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-2.10.0-py3-none-any.whl (660 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 4.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Using cached sacremoses-0.0.43.tar.gz (883 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages (from transformers) (4.46.0)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages (from transformers) (2.23.0)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading dataclasses-0.7-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages (from transformers) (3.0.12)\n",
      "Collecting tokenizers==0.7.0\n",
      "  Downloading tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Downloading regex-2020.5.14-cp36-cp36m-manylinux2010_x86_64.whl (675 kB)\n",
      "\u001b[K     |████████████████████████████████| 675 kB 67.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages (from transformers) (1.18.4)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: click in /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/envs/aws_neuron_pytorch_p36/lib/python3.6/site-packages (from requests->transformers) (2020.4.5.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=0561663ac8d286ebb56c247d0aa8a6485fa26e080fc1e40fd2f1b06e80ad4123\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/49/25/98/cdea9c79b2d9a22ccc59540b1784b67f06b633378e97f58da2\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sentencepiece, regex, sacremoses, dataclasses, tokenizers, transformers\n",
      "Successfully installed dataclasses-0.7 regex-2020.5.14 sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.7.0 transformers-2.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting BERTq1.py\n"
     ]
    }
   ],
   "source": [
    "%%file BERTq1.py\n",
    "from mrjob.job import MRJob\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "import pandas as pd\n",
    "import json \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BERTq1(MRJob):\n",
    "\n",
    "    def mapper(self,_,line):\n",
    "        line=json.loads(line)\n",
    "        paper_id=line[\"paper_id\"]\n",
    "        text_dict=line[\"text_dict\"]\n",
    "        for key in text_dict.keys():\n",
    "            content = text_dict[key]\n",
    "            com_id=str(paper_id)+str(key)\n",
    "            yield com_id, str(content)\n",
    "\n",
    "    def reducer(self, com_id, content):\n",
    "        question=\"What is the best method to combat the hypercoagulable state seen in COVID-19 ?\"\n",
    "        answer_text=str(content)        \n",
    "        \n",
    "        input_ids = tokenizer.encode(question, answer_text,max_length=500)\n",
    "\n",
    "        # Report how long the input sequence is.\n",
    "        #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "        # ======== Set Segment IDs ========\n",
    "        # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "        sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "        # The number of segment A tokens includes the [SEP] token istelf.\n",
    "        num_seg_a = sep_index + 1\n",
    "\n",
    "        # The remainder are segment B.\n",
    "        num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "        # Construct the list of 0s and 1s.\n",
    "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "        # There should be a segment_id for every input token.\n",
    "        assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "        # ======== Evaluate ========\n",
    "        # Run our example question through the model.\n",
    "        start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                        token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n",
    "\n",
    "        # ======== Reconstruct Answer ========\n",
    "        # Find the tokens with the highest `start` and `end` scores.\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores)\n",
    "\n",
    "\n",
    "        # Get the string versions of the input tokens.\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        # Start with the first token.\n",
    "        answer = tokens[answer_start]\n",
    "\n",
    "        # Select the remaining answer tokens and join them with whitespace.\n",
    "        for i in range(answer_start + 1, answer_end + 1):\n",
    "\n",
    "            # If it's a subword token, then recombine it with the previous token.\n",
    "            if tokens[i][0:2] == '##':\n",
    "                answer += tokens[i][2:]\n",
    "\n",
    "            # Otherwise, add a space then the token.\n",
    "            else:\n",
    "                answer += ' ' + tokens[i]\n",
    "\n",
    "        s_scores = start_scores.detach().numpy().flatten()\n",
    "        e_scores = end_scores.detach().numpy().flatten()  \n",
    "\n",
    "        if (answer!=\"[CLS]\") & (float(s_scores)>0)& (len(answer)>0):\n",
    "            yield com_id, answer\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BERTq1.run()\n",
    "    \n",
    "\n",
    "# !python BERTq1.py sm_df.txt > BERTq1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run program\n",
    "\n",
    "- Specify `config file`: --conf-path .mrjob.conf\n",
    "- Specify `input` and `output` file:  DataA1.csv > MRTotalAmountSpentByCustomer.txt\n",
    "- Specify `runner`:  -r emr \n",
    "- Run .py: !python MRTotalAmountSpentByCustomer\n",
    "\n",
    "### Explanation:\n",
    "- AWS automatically create `S3`, `EMR` instance for you \n",
    "- `EMR` instance will be deleted after running\n",
    "- Output can be seen on `S3`\n",
    "- Can specify other `runner` instead of `emr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run example on local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "Using s3://mrjob-84e4f2c1e3c7e976/tmp/ as our temp dir on S3\n",
      "INFO:mrjob.emr:Using s3://mrjob-84e4f2c1e3c7e976/tmp/ as our temp dir on S3\n",
      "Creating temp directory /tmp/BERTq1.ubuntu.20200525.225353.277976\n",
      "INFO:mrjob.runner:Creating temp directory /tmp/BERTq1.ubuntu.20200525.225353.277976\n",
      "writing master bootstrap script to /tmp/BERTq1.ubuntu.20200525.225353.277976/b.sh\n",
      "INFO:mrjob.cloud:writing master bootstrap script to /tmp/BERTq1.ubuntu.20200525.225353.277976/b.sh\n",
      "uploading working dir files to s3://mrjob-84e4f2c1e3c7e976/tmp/BERTq1.ubuntu.20200525.225353.277976/files/wd...\n",
      "INFO:mrjob.runner:uploading working dir files to s3://mrjob-84e4f2c1e3c7e976/tmp/BERTq1.ubuntu.20200525.225353.277976/files/wd...\n",
      "Copying other local files to s3://mrjob-84e4f2c1e3c7e976/tmp/BERTq1.ubuntu.20200525.225353.277976/files/\n",
      "INFO:mrjob.runner:Copying other local files to s3://mrjob-84e4f2c1e3c7e976/tmp/BERTq1.ubuntu.20200525.225353.277976/files/\n",
      "Created new cluster j-31CEQSSLTRD5R\n",
      "INFO:mrjob.emr:Created new cluster j-31CEQSSLTRD5R\n",
      "Added EMR tags to cluster j-31CEQSSLTRD5R: __mrjob_label=BERTq1, __mrjob_owner=ubuntu, __mrjob_version=0.7.2\n",
      "INFO:mrjob.emr:Added EMR tags to cluster j-31CEQSSLTRD5R: __mrjob_label=BERTq1, __mrjob_owner=ubuntu, __mrjob_version=0.7.2\n",
      "Waiting for Step 1 of 1 (s-1VT5TNB4QU1WJ) to complete...\n",
      "INFO:mrjob.emr:Waiting for Step 1 of 1 (s-1VT5TNB4QU1WJ) to complete...\n",
      "  PENDING (cluster is STARTING)\n",
      "INFO:mrjob.emr:  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "INFO:mrjob.emr:  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "INFO:mrjob.emr:  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "INFO:mrjob.emr:  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "INFO:mrjob.emr:  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "INFO:mrjob.emr:  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "INFO:mrjob.emr:  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  master node is ec2-34-214-222-200.us-west-2.compute.amazonaws.com\n",
      "INFO:mrjob.emr:  master node is ec2-34-214-222-200.us-west-2.compute.amazonaws.com\n",
      "  Connect to resource manager at: http://localhost:40038/cluster\n",
      "INFO:mrjob.cloud:  Connect to resource manager at: http://localhost:40038/cluster\n",
      "  RUNNING for 0:00:05\n",
      "INFO:mrjob.emr:  RUNNING for 0:00:05\n",
      "  FAILED\n",
      "INFO:mrjob.emr:  FAILED\n",
      "Cluster j-31CEQSSLTRD5R was TERMINATED_WITH_ERRORS: Shut down as step failed\n",
      "INFO:mrjob.emr:Cluster j-31CEQSSLTRD5R was TERMINATED_WITH_ERRORS: Shut down as step failed\n",
      "Attempting to fetch counters from logs...\n",
      "INFO:mrjob.logs.mixin:Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-1VT5TNB4QU1WJ on ec2-34-214-222-200.us-west-2.compute.amazonaws.com...\n",
      "INFO:mrjob.emr:Looking for step log in /mnt/var/log/hadoop/steps/s-1VT5TNB4QU1WJ on ec2-34-214-222-200.us-west-2.compute.amazonaws.com...\n",
      "Looking for step log in s3://mrjob-84e4f2c1e3c7e976/tmp/logs/j-31CEQSSLTRD5R/steps/s-1VT5TNB4QU1WJ...\n",
      "INFO:mrjob.emr:Looking for step log in s3://mrjob-84e4f2c1e3c7e976/tmp/logs/j-31CEQSSLTRD5R/steps/s-1VT5TNB4QU1WJ...\n",
      "  Parsing step log: s3://mrjob-84e4f2c1e3c7e976/tmp/logs/j-31CEQSSLTRD5R/steps/s-1VT5TNB4QU1WJ/syslog.gz\n",
      "INFO:mrjob.emr:  Parsing step log: s3://mrjob-84e4f2c1e3c7e976/tmp/logs/j-31CEQSSLTRD5R/steps/s-1VT5TNB4QU1WJ/syslog.gz\n",
      "Counters: 17\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=87\n",
      "\t\tFailed map tasks=164\n",
      "\t\tKilled map tasks=255\n",
      "\t\tKilled reduce tasks=126\n",
      "\t\tLaunched map tasks=246\n",
      "\t\tOther local map tasks=159\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6169843200\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=0\n",
      "\t\tTotal time spent by all map tasks (ms)=1606730\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=192807600\n",
      "\t\tTotal time spent by all reduce tasks (ms)=0\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=1606730\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "INFO:mrjob.logs.mixin:Counters: 17\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=87\n",
      "\t\tFailed map tasks=164\n",
      "\t\tKilled map tasks=255\n",
      "\t\tKilled reduce tasks=126\n",
      "\t\tLaunched map tasks=246\n",
      "\t\tOther local map tasks=159\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=6169843200\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=0\n",
      "\t\tTotal time spent by all map tasks (ms)=1606730\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=192807600\n",
      "\t\tTotal time spent by all reduce tasks (ms)=0\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=1606730\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "Scanning logs for probable cause of failure...\n",
      "INFO:mrjob.logs.mixin:Scanning logs for probable cause of failure...\n",
      "Looking for history log in hdfs:///tmp/hadoop-yarn/staging/history...\n",
      "INFO:mrjob.emr:Looking for history log in hdfs:///tmp/hadoop-yarn/staging/history...\n",
      "STDERR: Warning: Identity file /home/ubuntu/covid19-challenge/notebook/ka200510.pem not accessible: No such file or directory.\n",
      "ERROR:mrjob.fs.hadoop:STDERR: Warning: Identity file /home/ubuntu/covid19-challenge/notebook/ka200510.pem not accessible: No such file or directory.\n",
      "Looking for history log in /mnt/var/log/hadoop-mapreduce/history on ec2-34-214-222-200.us-west-2.compute.amazonaws.com...\n",
      "INFO:mrjob.emr:Looking for history log in /mnt/var/log/hadoop-mapreduce/history on ec2-34-214-222-200.us-west-2.compute.amazonaws.com...\n",
      "Looking for history log in s3://mrjob-84e4f2c1e3c7e976/tmp/logs/j-31CEQSSLTRD5R/hadoop-mapreduce/history...\n",
      "INFO:mrjob.emr:Looking for history log in s3://mrjob-84e4f2c1e3c7e976/tmp/logs/j-31CEQSSLTRD5R/hadoop-mapreduce/history...\n",
      "  Parsing history log: s3://mrjob-84e4f2c1e3c7e976/tmp/logs/j-31CEQSSLTRD5R/hadoop-mapreduce/history/2020/05/25/000000/job_1590447398559_0001-1590447485157-hadoop-streamjob3888115476649591614.jar-1590447525657-0-0-FAILED-default-1590447490429.jhist.gz\n",
      "INFO:mrjob.logs.mixin:  Parsing history log: s3://mrjob-84e4f2c1e3c7e976/tmp/logs/j-31CEQSSLTRD5R/hadoop-mapreduce/history/2020/05/25/000000/job_1590447398559_0001-1590447485157-hadoop-streamjob3888115476649591614.jar-1590447525657-0-0-FAILED-default-1590447490429.jhist.gz\n",
      "Looking for task logs in /mnt/var/log/hadoop-yarn/containers/application_1590447398559_0001 on ec2-34-214-222-200.us-west-2.compute.amazonaws.com and task/core nodes...\n",
      "INFO:mrjob.emr:Looking for task logs in /mnt/var/log/hadoop-yarn/containers/application_1590447398559_0001 on ec2-34-214-222-200.us-west-2.compute.amazonaws.com and task/core nodes...\n"
     ]
    }
   ],
   "source": [
    "!python  BERTq1.py sm_df_sample.txt > BERTq1.txt -r emr --conf-path .mrjob.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mochila\n",
    "import mochila\n",
    "email=email_send(subject = )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python MRTotalAmountSpentByCustomer.py DataA1.csv > MRTotalAmountSpentByCustomer.txt -r emr --conf-path .mrjob.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Running on `Local`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs specified for local runner\n",
      "Creating temp directory /tmp/MRTotalAmountSpentByCustomer.ubuntu.20200523.220810.595798\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/MRTotalAmountSpentByCustomer.ubuntu.20200523.220810.595798/output\n",
      "Streaming final output from /tmp/MRTotalAmountSpentByCustomer.ubuntu.20200523.220810.595798/output...\n",
      "Removing temp directory /tmp/MRTotalAmountSpentByCustomer.ubuntu.20200523.220810.595798...\n"
     ]
    }
   ],
   "source": [
    "!python MRTotalAmountSpentByCustomer.py DataA1.csv > MRTotalAmountSpentByCustomer.txt -r local --conf-path .mrjob.conf"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Environment (conda_aws_neuron_pytorch_p36)",
   "language": "python",
   "name": "conda_aws_neuron_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
