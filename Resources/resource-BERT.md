# BERT

B: bidirectional
E: encoder
R: representations
T: transformaers

BERT: 
- BERT: Bidirectioanl encoder representations from transformers
- A new method of pre-training language representations which obatains great results on NLP tasks.
- BERT is Google's October 2019 update to the algorithm
([google improvements examples](https://blog.google/products/search/search-language-understanding-bert))
- open source in 2018

- Can help computers understand language like humans

Why important:
- to train language models based on the entire set of words in a sentence/ query rather than traditionally training on the ordered sequence of words
- allows the language model to learn word **context** based on surrounding words rather than just the word precdes or follows it


Optimization:
- You cannot optimize BERT specifically because there is nothing to optimize

## EAT

E: expertise
A: authority
T: trustworthiness

- EAT is not an algorithms but a guideline for Quality Raters to assign quality scores to search results for queries [see guidelines](https://static.googleusercontent.com/media/guidelines.raterhub.com/en//searchqualityevaluatorguidelines.pdf).




[ref video](https://www.youtube.com/watch?v=9zvsnzgHiWM)
