{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MRJob on EMR (Elastic Map Reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install mrjob\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!pip install mrjob\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create configuration file for mrjob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .mrjob.conf\n"
     ]
    }
   ],
   "source": [
    "%%file .mrjob.conf\n",
    "\n",
    "# http://mrjob.readthedocs.io/en/stable/guides/emr-opts.html\n",
    "\n",
    "runners:\n",
    "  dataproc:\n",
    "    instance_type: n1-standard-2\n",
    "    #instance_type: e2-standard-8\n",
    "    num_core_instances: 2\n",
    "  local:\n",
    "    #instance_type: e2-standard-8\n",
    "    #num_core_instances: 8\n",
    "  emr:\n",
    "    aws_access_key_id: AKIAI5ELHG5OLN757ENQ\n",
    "    aws_secret_access_key: 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\n",
    "    ec2_key_pair: ka200510\n",
    "    ec2_key_pair_file: /home/ubuntu/covid19-challenge/notebook/ka200510.pem\n",
    "    region: us-west-2 # http://docs.aws.amazon.com/general/latest/gr/rande.html\n",
    "    master_instance_type: m5a.8xlarge # https://aws.amazon.com/emr/pricing/\n",
    "    instance_type: m5a.8xlarge\n",
    "    num_core_instances: 0\n",
    "    ssh_tunnel: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export GOOGLE_APPLICATION_CREDENTIALS=\"/home/jupyter/covid19-challenge/notebook mindful-carport-278217-556456672726.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create program: MRJob .py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: \n",
    "- Q1: `What is the best method to combat the hypercoagulable state seen in COVID-19`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try JSON to turn string to dict : Failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list=[]\n",
    "with open(\"sm_df.txt\", 'r') as f:\n",
    "    for line in f:\n",
    "        dict_list.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_samp=dict_list[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['paper_id', 'text_dict'])\n",
      "dict_keys(['paper_id', 'text_dict'])\n"
     ]
    }
   ],
   "source": [
    "for i in dict_samp:\n",
    "    print(i.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2a667552f5924857d6eaf9a8104e5f92e236dde9'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_example[\"paper_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_example[\"text_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guide for their recognition and prevention, stirred interest and attention towards EIDs and opened doors for more research, funding and publications (IOM 1992) . The establishment of the Program for Monitoring Emerging Diseases (ProMED), devoted to rapidly disseminate information on infectious disease outbreaks and promote communication and collaboration amongst the international infectious diseases community during EID outbreaks, shortly followed this conference and publication. The Centres for Disease Control (CDC), in 1995, launched the Emerging Infectious Diseases journal, dedicated to rapidly disseminate reliable information on the emergence, prevention and elimination of EIDs.Preceding these developments in EID dedicated programs and journals, research centres spread across the globe were developing vaccines and conducting trials to prevent various emerging infectious threats throughout history. The Institut Pasteur (IP), established in Paris in 1888, is one of the oldest of such centres for research on EIDs. The Institut, which conducts research at the frontline of EID outbreaks, aims to make discoveries to prevent, control, diagnose and treat EIDs. Soon after its creation, in 1896, Emile Marchoux established a microbiology laboratory in Saint-Louis, Senegal -the site where the first outbreak of yellow fever was reported in 1778 (Augustine 1909) . This laboratory in Saint-Louis studied the epidemiology of malaria and sleeping sickness, and developed vaccines against smallpox, rabies and the plague. The laboratory was later transferred to Dakar in 1924 following an agreement with IP Paris, and was renamed the Institut Pasteur of Dakar. It is now a World Health Organisation (WHO) reference centre and a partner in the WHO Global Outbreak Alert and Response Network (GOARN). In addition to Senegal, Institut Pasteur is present in 9 other African countries: Guinea Conakry, Ivory Coast, Niger, Cameroon, Central African Republic, Madagascar, Morocco, Algeria and Tunisia; making the Institut Pasteur International Network a major player in almost every EID outbreak today.In the 1920s, the West African Yellow Fever Commission was formed, run by a colonial research team from the Rockefeller Foundation. The Foundation build research centres and laboratories in Nigeria (Yaba, Lagos) and Ghana where significant studies and discoveries were made including the isolation of the yellow fever virus (Bigon 2014) The most developed of these early research establishments was perhaps the Medical Research Council (MRC) Unit in The Gambia. Established in 1947, it has since led infectious disease and public health research, making significant contributions in the prevention of endemic and emerging pathologies in the region including malaria, tuberculosis and Haemophilus influenza. It remains a beacon of excellence in bench, field and clinical research on infectious diseases, training and hosting some of Africa's leading scientists and producing a significant proportion of the region's research output. During the peak of the HIV epidemic in West Africa, the Unit supported the establishment of a field station in Caio, Guinea Bissau dedicated to the study and prevention of HIV infection. The MRC Unit in The Gambia is now incorporated within the London School of Hygiene & Tropical Medicine, and continues to forge partnerships with several other institutions in the West African region.In the mid-1950s to 1970s, some African nations through national assembly acts, created what are perhaps the oldest indigenous government supported research centres in sub-Saharan Africa, tasked purposely to improve the health and quality of life 1956 and 1979 . KEMRI, in 1989 , went on to form a partnership with the Wellcome Trust and the University of Oxford to create the KEMRI-Wellcome Trust Research Programme. This world-renowned health research unit trains scientific leaders and champions innovative and novel ideas to improve healthcare in Africa. Its Genomics and Infectious Diseases platform studies the transmission of infectious diseases, including EIDs, in order to inform disease outbreak policies and understand the evolution of resistance.Both the NIMR and the Noguchi Memorial Institute for Medical research, respectively the leading medical research institutes in Nigeria and Ghana, focus on preventing and controlling infectious diseases of the highest priority to their respective populations and the regions. Their research work includes studies to control and eradicate malaria, onchocerciasis, schistosomiasis, childhood diarrhoeal diseases, tuberculosis, HIV, leprosy, guinea worm, Ebola virus disease, yellow fever and Lassa fever.In 1988, at the height of the HIV epidemic in sub-Saharan Africa, an agreement between the governments of Uganda and Britain led to the establishment of the MRC Unit in Uganda. This unit, hosted by the Uganda Virus Research Institute (UVRI), primarily conducts research to investigate the determinants of HIV and related infections, their subsequent disease progression, and evaluate new preventive strategies and interventions for their control in Africa. The UVRI also hosts a CDC supported laboratory for rapid diagnosis of haemorrhagic fever viruses. This diagnostic infrastructure, coupled with other public health measures, has helped in a more rapid detection and control of outbreaks in Uganda. Now a hotspot for Ebola experts, Uganda and its centres of excellence have assisted in controlling outbreaks within the region.In 2004, the CDC in coordination with the WHO, established a Global Disease Detection (GDD) Regional Centre in Kenya, co-located with KEMRI. The GDD regional centre connects regions and countries throughout Africa, and assists in detecting and responding to disease outbreaks. A similar GDD regional centre was established in Thailand that year to address the complexity of EIDs in the Asia-Pacific region. Between 2005 and 2011, eight additional GDD centres were established in Bangladesh, Guatemala, Kazakhstan, China, Egypt, India, South Africa and Georgia, with all centres aiming to detect and stop disease outbreaks at their source thereby limiting spread and preventing epidemics. These centres, combined, have responded to hundreds of outbreaks; detected and identified scores of novel strains and pathogens; and improved regional capacity in personnel, diagnostics and surveillance.In addition to the GDD program, the WHO created the Global Outbreak Alert and Response Network (GOARN) to facilitate harnessing of international resources at the request of WHO member states during outbreaks. This network augments country and regional responses to ongoing or potential public health emergencies. GOARN, in 2014, facilitated the deployment of nearly 900 experts to West Africa during the Ebola outbreak. The WHO and CDC publish recommendations to guide the coordination of national surveillance and response standards for EIDs and communicable diseases.\n",
      "   \n",
      "An EID outbreak occurs in stages, the first of which is often the introduction of the new pathogen into the host population, or in the case of a previously existing pathogen, its establishment and dissemination within the new population and beyond. Whilst most of the EID outbreaks in recent history have resulted from zoonotic infections, only a few of the many zoonotic pathogens that periodically infect humans become adept at transmitting or propagating themselves. Human activity, however, is making this transition increasingly easy by creating efficient pathways for pathogen transmission around the globe. Woolhouse and Gaunt (2007) , following careful examination of dozens of newly identified human pathogens, noted four characteristics that they expect will describe most EIDs:1. Caused by an RNA virus 2. Caused by pathogen with non-human (animal) reservoir 3. Caused by pathogen with a broad host range 4. Have potential for human-to-human transmissionIn the presence of these four characteristics, changes in climate and human behaviour are key elements in the promotion of most EID outbreaks. With the on-going global ecological change, it is estimated that new pathogens and EIDs will continue to be introduced into the human population at or near the rate of three per year (Woolhouse and Gaunt 2007) . Box 3.1 summarises the key factors that promote EIDs.\n",
      "   \n",
      "Despite the systems in place for preventing EIDs, the frequency of outbreaks have remained stable over the past decades. Between 1980 and 2017, more than two dozen EID outbreaks have been reports. Whilst some of these were re-emerging pathogens, there were more than ten newly emerging infectious diseases reported for the first time in humans during this time (Table 3 .1). Unsurprisingly, most of these newly reported EIDs are new zoonotic viral infections or mutated forms of previously circulating viruses. Discussions of the epidemiology, the pathogen characteristics, and clinical features of these EIDs is beyond the scope of this chapter. The increase in EID outbreaks in part reflects advances in medical and technological capabilities to detect and diagnose infections. However, increased globalisation, changes in global climate and human behaviour, natural and man-made disasters (poverty, war), weak healthcare systems, and lapses in public health measures all contribute to the increase in EIDs. The importance of these factors is borne in the fact that the majority of EIDs represent changes in the epidemiology, virulence or adaptation of previously known pathogens, with novel pathogens accounting for a minority of EID outbreaks.\n",
      "   \n",
      "Human activities, including war, poor health systems, international travel, increasing population, and dependence on livestock for food and labour, have promoted the emergence, expansion, and rapid spread of EIDs. With the majority of recent EID outbreaks caused by zoonotic viral infections, initiatives and response mechanisms have been established to curb, control and prevent the emergence and spread of animal and botanical EIDs that could have adverse impact on human lives and livelihood: \n",
      "   \n",
      "As previously mentioned, the WHO had set up GDD Centers in Kenya and other African countries. There are 12 offices in all, one of which is in South Africa. The One Health Program was founded as a collaborative effort to bring the best possible conditions for health to people, animals and the environment. It pulls together professionals from international, national and local regions in agreement with the objectives of the WHO. The One Health Program focuses on public health and zoonotic diseases in an effort to curtail zoonotic outbreaks that affect morbidity and mortality of people and animals alike. The South Africa Regional Global Disease Detection Center (GDD) provides support for the training and human capital development to prepare health officials in the detection and response to zoonotic diseases according to the GHS initiatives. The strengthening of laboratory surveillance, research programs and an enhanced surveillance infrastructure are objectives of the GDD and One Health. Priority Zoonotic diseases are:• Viral, endemic zoonotic diseases: Rabies, Ebola, Marburg • Bacterial diseases: Anthrax, Brucellosis, Bovine Tuberculosis. Leptospirosis, Rickettsial diseases and food borne bacteria like Salmonella • WHO neglected diseases: Toxoplasmosis, Cysticercosis, Cryptosporidium\n",
      "   \n",
      "Rinderpest is German for \"cattle plague\". It is a highly infectious viral disease that can affect domesticated and wild, cloven-hoofed mammals. African cattle and buffalo are extremely susceptible to the disease while sheep and goats experience a mild form that is not as acute, resulting in lower fatalities. In susceptible animal, Rinderpest infection has a fatality rate greater than 90% and a mortality rate of 100%. In the 1890's, an outbreak of Rinderpest South of the River Zambezi killed about 5.2 million cattle, sheep, goats and oxen, and an unknown number of wild giraffe, buffalo and wildebeest. This Rinderpest epidemic, happening at a time of much historical tumult, was the most devastating epidemic to hit southern Africa in the late nineteenth century. There is no conclusive evidence on how Rinderpest was brought to Africa. It is thought to have been brought through Indian cattle by Italian colonial masters who introduced the cattle to the Horn of Africa. The disease is spread through ingested contaminated matter and/or through direct contact. Endemic areas have caused animals to develop immunity through exposure or vaccinations. In sub-Saharan Africa, Rinderpest outbreaks resulted in the death of 90% of domestic oxen. The resulting famine, both from loss of livestock and diminished agricultural produce, led to significant mortality among Ethiopian and Masai populations. European nations gradually eliminated Rinderpest in the early twentieth century through surveillance, early detection and culling of sick and exposed animals. It however continued to afflict developing African and Asian countries well into the second half of the twentieth century, re-emerging after several eradication campaigns were prematurely shut down due to mistaken belief that the virus had been eradicated. It was not until 2011 that Rinderpest was eradicated, marking one of the greatest veterinary achievements in recent history.\n",
      "   \n",
      "Rift Valley Fever (RVF) is a virus from the Phlebovirus genus, first identified in the Rift Valley of Kenya in 1931. RVF primarily infects animals but has the ability to also affect humans. Since 1931, there have been repeated outbreaks in sub-Saharan Africa. Infected cattle along the River Nile and its irrigation system caused an outbreak of RVF in Egypt in 1977 that killed approximately 600 people. In 1987 there was an outbreak in West Africa that was connected with the Senegal River Project construction. In 1997-98, there was another outbreak due to El Nino in Tanzania, Kenya and Somalia amidst extensive flooding. Humans can get RVF through the bites of infected insects and contact with blood of infected animals.\n",
      "   \n",
      "Also known as Ovine Rinderpest, PPR is a disease that affects smaller ruminants (primarily sheep and goats) in Central and Southern Africa, North Africa, the Middle East and the Indian subcontinent. It is a highly contagious and fatal disease, making its impact on livestock and human livelihood very important, and its global eradication an urgent need (Baron et al. 2011 ).\n",
      "   \n",
      "Data and countermeasures to control and treat most EIDs are insufficient, often resulting in significant morbidity and mortality during outbreaks. This is especially problematic in resource-limited countries, where healthcare infrastructure for disease surveillance and rapid response to emerging outbreaks are weak. This urgent problem was brought to the forefront during the Ebola virus outbreak of 2014 in West Africa. As a result, the WHO has, since December 2015, maintained a list of EIDs that pose major public health threats and for which there are no effective countermeasures, and advocates the prioritization of these EIDs for research and vaccine/drug development.An effective global prevention of EID outbreaks will require multidisciplinary collaboration and will include the following:\n",
      "   \n",
      "It has been 50 years or more since many African countries became independent. During this period, Africa has experienced numerous disease outbreaks, many of which have become endemic and uncontrolled. It is sorrowful and painful to note that Africa remains the verdant pasture for EIDs despite global improvements in disease surveillance systems and better reliable laboratory diagnostic facilities. Behind Africa's failure to successfully control EID outbreaks is the pervasive state of poor governance in many African countries, with governments continuing to pay lip service to health issues including disease control, and African healthcare systems remaining weak despite repeated EID outbreaks. In the absence of good governance, priorities get misplaced and an atmosphere of poor planning and corruption is easily promoted resulting in poor allocation and mismanagement of funds for disease control and surveillance. It is essential for African governments to note that a healthy population is a sine qua non for orderly economic and social development, and that EID outbreaks negatively impact national economies and other development plans. It is only through good governance and political/economic stability that African countries can improve health security and prevent EIDs and preventable loss of life.\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "for key in one_example[\"text_dict\"].keys():\n",
    "    print(one_example[\"text_dict\"][key])\n",
    "    print(\"   \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReduce for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 0: Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting BERTq1.py\n"
     ]
    }
   ],
   "source": [
    "%%file BERTq1.py\n",
    "#!pip install transformers\n",
    "from mrjob.job import MRJob\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "import pandas as pd\n",
    "import json \n",
    "\n",
    "Q_example=\"What is the best method to combat the hypercoagulable state seen in COVID-19 ?\"\n",
    "\n",
    "\n",
    "def answer_question(question, answer_text):\n",
    "    '''\n",
    "    Takes a `question` string and an `answer_text` string (which contains the\n",
    "    answer), and identifies the words within the `answer_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # ======== Tokenize ========\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    input_ids = tokenizer.encode(question, answer_text,max_length=500\n",
    "                                )\n",
    "\n",
    "    # Report how long the input sequence is.\n",
    "    #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "    # ======== Set Segment IDs ========\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # ======== Evaluate ========\n",
    "    # Run our example question through the model.\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n",
    "\n",
    "    # ======== Reconstruct Answer ========\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "\n",
    "\n",
    "    # Get the string versions of the input tokens.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "\n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "\n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "\n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    s_scores = start_scores.detach().numpy().flatten()\n",
    "    e_scores = end_scores.detach().numpy().flatten()\n",
    "    #print('score:'+(start_scores)+\"; \"+str(end_scores))\n",
    "    #print('score:'+str(max(s_scores))+\"; \"+str(min(e_scores)))\n",
    "    #print(str(tensor[torch.argmax(start_scores)]))\n",
    "    #print('Answer: \"' + answer + '\"')\n",
    "    return [answer,str(max(s_scores)),len(input_ids)]\n",
    "\n",
    "\n",
    "class BERTq1(MRJob):\n",
    "\n",
    "    def mapper(self,_,line):\n",
    "        line=json.loads(line)\n",
    "        paper_id=line[\"paper_id\"]\n",
    "        text_dict=line[\"text_dict\"]\n",
    "        for key in text_dict.keys():\n",
    "            content = text_dict[key]\n",
    "            yield paper_id, content\n",
    "\n",
    "    def reducer(self, paper_id, content):\n",
    "        print(paper_id)\n",
    "        result=answer_question(Q_example,content)\n",
    "        answer=result[0]\n",
    "        #s_score=result[1]\n",
    "        #token_len=result[2]\n",
    "        if (answer!=\"[CLS]\") & (float(s_score)>0)& (len(answer)>0):\n",
    "            yield paper_id, answer\n",
    "\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BERTq1.run()\n",
    "    \n",
    "\n",
    "# !python BERTq1.py sm_df.txt > BERTq1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting BERTq1.py\n"
     ]
    }
   ],
   "source": [
    "%%file BERTq1.py\n",
    "#!pip install transformers\n",
    "from mrjob.job import MRJob\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "import pandas as pd\n",
    "import json \n",
    "\n",
    "Q_example=\"What is the best method to combat the hypercoagulable state seen in COVID-19 ?\"\n",
    "\n",
    "class BERTq1(MRJob):\n",
    "\n",
    "    def mapper(self,_,line):\n",
    "        line=json.loads(line)\n",
    "        paper_id=line[\"paper_id\"]\n",
    "        text_dict=line[\"text_dict\"]\n",
    "        for key in text_dict.keys():\n",
    "            content = text_dict[key]\n",
    "            yield paper_id, key, content\n",
    "\n",
    "    def reducer(self, paper_id, key, content):\n",
    "        print(paper_id)\n",
    "        result=answer_question(Q_example,content)\n",
    "        answer=result[0]\n",
    "        s_score=result[1]\n",
    "        token_len=result[2]\n",
    "        if (answer!=\"[CLS]\") & (float(s_score)>0)& (len(answer)>0):\n",
    "            yield paper_id, paper_id, answer, s_score, token_len\n",
    "\n",
    "        \n",
    "    def answer_question(question, answer_text):\n",
    "        '''\n",
    "        Takes a `question` string and an `answer_text` string (which contains the\n",
    "        answer), and identifies the words within the `answer_text` that are the\n",
    "        answer. Prints them out.\n",
    "        '''\n",
    "        # ======== Tokenize ========\n",
    "        # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "        input_ids = tokenizer.encode(question, answer_text,max_length=500\n",
    "                                    )\n",
    "\n",
    "        # Report how long the input sequence is.\n",
    "        #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "        # ======== Set Segment IDs ========\n",
    "        # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "        sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "        # The number of segment A tokens includes the [SEP] token istelf.\n",
    "        num_seg_a = sep_index + 1\n",
    "\n",
    "        # The remainder are segment B.\n",
    "        num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "        # Construct the list of 0s and 1s.\n",
    "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "        # There should be a segment_id for every input token.\n",
    "        assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "        # ======== Evaluate ========\n",
    "        # Run our example question through the model.\n",
    "        start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                        token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n",
    "\n",
    "        # ======== Reconstruct Answer ========\n",
    "        # Find the tokens with the highest `start` and `end` scores.\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores)\n",
    "\n",
    "\n",
    "        # Get the string versions of the input tokens.\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        # Start with the first token.\n",
    "        answer = tokens[answer_start]\n",
    "\n",
    "        # Select the remaining answer tokens and join them with whitespace.\n",
    "        for i in range(answer_start + 1, answer_end + 1):\n",
    "\n",
    "            # If it's a subword token, then recombine it with the previous token.\n",
    "            if tokens[i][0:2] == '##':\n",
    "                answer += tokens[i][2:]\n",
    "\n",
    "            # Otherwise, add a space then the token.\n",
    "            else:\n",
    "                answer += ' ' + tokens[i]\n",
    "\n",
    "        s_scores = start_scores.detach().numpy().flatten()\n",
    "        e_scores = end_scores.detach().numpy().flatten()\n",
    "        #print('score:'+(start_scores)+\"; \"+str(end_scores))\n",
    "        #print('score:'+str(max(s_scores))+\"; \"+str(min(e_scores)))\n",
    "        #print(str(tensor[torch.argmax(start_scores)]))\n",
    "        #print('Answer: \"' + answer + '\"')\n",
    "        return [answer,str(max(s_scores)),len(input_ids)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BERTq1.run()\n",
    "    \n",
    "\n",
    "# !python BERTq1.py sm_df.txt > BERTq1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file BERTq1.py\n",
    "#!pip install transformers\n",
    "from mrjob.job import MRJob\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "import pandas as pd\n",
    "import json \n",
    "\n",
    "Q_example=\"What is the best method to combat the hypercoagulable state seen in COVID-19?\"\n",
    "\n",
    "class BERTq1(MRJob):\n",
    "\n",
    "    def mapper(self,_,line):\n",
    "        line=json.loads(line)\n",
    "        paper_id=line[\"paper_id\"]\n",
    "        text_dict=line[\"text_dict\"]\n",
    "        yield paper_id, str(json.dumps(text_dict)).join(\"\\n\")\n",
    "\n",
    "    def reducer(self, paper_id, dct):\n",
    "        dct=json.loads(dct)\n",
    "        for key in dct.keys():\n",
    "            print(paper_id)\n",
    "            answer=answer_question(Q_example,dct[key])[0]\n",
    "            s_score=answer_question(Q_example,dct[key])[1]\n",
    "            token_len=answer_question(Q_example,dct[key])[2]\n",
    "            if (answer!=\"[CLS]\") & (float(s_score)>0)& (len(answer)>0):\n",
    "                yield paper_id, answer, s_score, token_len\n",
    "\n",
    "        \n",
    "    def answer_question(question, answer_text):\n",
    "        '''\n",
    "        Takes a `question` string and an `answer_text` string (which contains the\n",
    "        answer), and identifies the words within the `answer_text` that are the\n",
    "        answer. Prints them out.\n",
    "        '''\n",
    "        # ======== Tokenize ========\n",
    "        # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "        input_ids = tokenizer.encode(question, answer_text,max_length=500\n",
    "                                    )\n",
    "\n",
    "        # Report how long the input sequence is.\n",
    "        #print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "        # ======== Set Segment IDs ========\n",
    "        # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "        sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "        # The number of segment A tokens includes the [SEP] token istelf.\n",
    "        num_seg_a = sep_index + 1\n",
    "\n",
    "        # The remainder are segment B.\n",
    "        num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "        # Construct the list of 0s and 1s.\n",
    "        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "        # There should be a segment_id for every input token.\n",
    "        assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "        # ======== Evaluate ========\n",
    "        # Run our example question through the model.\n",
    "        start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                        token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n",
    "\n",
    "        # ======== Reconstruct Answer ========\n",
    "        # Find the tokens with the highest `start` and `end` scores.\n",
    "        answer_start = torch.argmax(start_scores)\n",
    "        answer_end = torch.argmax(end_scores)\n",
    "\n",
    "\n",
    "        # Get the string versions of the input tokens.\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "        # Start with the first token.\n",
    "        answer = tokens[answer_start]\n",
    "\n",
    "        # Select the remaining answer tokens and join them with whitespace.\n",
    "        for i in range(answer_start + 1, answer_end + 1):\n",
    "\n",
    "            # If it's a subword token, then recombine it with the previous token.\n",
    "            if tokens[i][0:2] == '##':\n",
    "                answer += tokens[i][2:]\n",
    "\n",
    "            # Otherwise, add a space then the token.\n",
    "            else:\n",
    "                answer += ' ' + tokens[i]\n",
    "\n",
    "        s_scores = start_scores.detach().numpy().flatten()\n",
    "        e_scores = end_scores.detach().numpy().flatten()\n",
    "        #print('score:'+(start_scores)+\"; \"+str(end_scores))\n",
    "        #print('score:'+str(max(s_scores))+\"; \"+str(min(e_scores)))\n",
    "        #print(str(tensor[torch.argmax(start_scores)]))\n",
    "        #print('Answer: \"' + answer + '\"')\n",
    "        return [answer,str(max(s_scores)),len(input_ids)]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    BERTq1.run()\n",
    "    \n",
    "\n",
    "# !python BERTq1.py sm_df.txt > BERTq1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run program\n",
    "\n",
    "- Specify `config file`: --conf-path .mrjob.conf\n",
    "- Specify `input` and `output` file:  DataA1.csv > MRTotalAmountSpentByCustomer.txt\n",
    "- Specify `runner`:  -r emr \n",
    "- Run .py: !python MRTotalAmountSpentByCustomer\n",
    "\n",
    "### Explanation:\n",
    "- AWS automatically create `S3`, `EMR` instance for you \n",
    "- `EMR` instance will be deleted after running\n",
    "- Output can be seen on `S3`\n",
    "- Can specify other `runner` instead of `emr`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run example on local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs specified for local runner\n",
      "Creating temp directory /tmp/BERTq1.jupyter.20200525.063416.884684\n",
      "Running step 1 of 1...\n",
      "\n",
      "Probable cause of failure:\n",
      "\n",
      "+ /opt/conda/bin/python BERTq1.py --step-num=0 --reducer\n",
      "Traceback (most recent call last):\n",
      "  File \"BERTq1.py\", line 100, in <module>\n",
      "    BERTq1.run()\n",
      "  File \"/tmp/BERTq1.jupyter.20200525.063416.884684/step/000/reducer/00000/wd/mrjob.zip/mrjob/job.py\", line 616, in run\n",
      "  File \"/tmp/BERTq1.jupyter.20200525.063416.884684/step/000/reducer/00000/wd/mrjob.zip/mrjob/job.py\", line 681, in execute\n",
      "  File \"/tmp/BERTq1.jupyter.20200525.063416.884684/step/000/reducer/00000/wd/mrjob.zip/mrjob/job.py\", line 795, in run_reducer\n",
      "  File \"/tmp/BERTq1.jupyter.20200525.063416.884684/step/000/reducer/00000/wd/mrjob.zip/mrjob/job.py\", line 866, in reduce_pairs\n",
      "  File \"/tmp/BERTq1.jupyter.20200525.063416.884684/step/000/reducer/00000/wd/mrjob.zip/mrjob/job.py\", line 889, in _combine_or_reduce_pairs\n",
      "  File \"BERTq1.py\", line 25, in reducer\n",
      "    result=answer_question(Q_example,content)\n",
      "NameError: name 'answer_question' is not defined\n",
      "\n",
      "(from lines 7-18 of /tmp/BERTq1.jupyter.20200525.063416.884684/step/000/reducer/00000/stderr)\n",
      "\n",
      "while reading input from /tmp/BERTq1.jupyter.20200525.063416.884684/step/000/reducer/00000/input\n",
      "\n",
      "\n",
      "Step 1 of 1 failed: Command '['/bin/sh', '-ex', 'setup-wrapper.sh', '/opt/conda/bin/python', 'BERTq1.py', '--step-num=0', '--reducer']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "!python BERTq1.py sm_df.txt > BERTq1.txt -r local --conf-path .mrjob.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python MRTotalAmountSpentByCustomer.py DataA1.csv > MRTotalAmountSpentByCustomer.txt -r emr --conf-path .mrjob.conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See Running on `Local`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs specified for local runner\n",
      "Creating temp directory /tmp/MRTotalAmountSpentByCustomer.ubuntu.20200523.220810.595798\n",
      "Running step 1 of 1...\n",
      "job output is in /tmp/MRTotalAmountSpentByCustomer.ubuntu.20200523.220810.595798/output\n",
      "Streaming final output from /tmp/MRTotalAmountSpentByCustomer.ubuntu.20200523.220810.595798/output...\n",
      "Removing temp directory /tmp/MRTotalAmountSpentByCustomer.ubuntu.20200523.220810.595798...\n"
     ]
    }
   ],
   "source": [
    "!python MRTotalAmountSpentByCustomer.py DataA1.csv > MRTotalAmountSpentByCustomer.txt -r local --conf-path .mrjob.conf"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
