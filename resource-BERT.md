# BERT

B: bidirectional
E: encoder
R: representations
T: transformaers

BERT: 
- BERT: Bidirectioanl encoder representations from transformers
- A new method of pre-training language representations which obatains great results on NLP tasks.
- BERT is Google's October 2019 update to the algorithm
[link](https://blog.google/products/search/search-language-understanding-bert)
- Can help computers understand language like humans

Why important:
- to train language models based on the entire set of words in a sentence/ query rather than traditionally training on the ordered sequence of words
- allows the language model to learn word context based on surrounding words rather than just the word precdes or follows it

This is from the webinar "BERT: Googleâ€™s Latest Search Algorithm to Better Understand Natural Language." Watch the full webinar here: https://youtu.be/9zvsnzgHiWM

For additional insights visit https://www.lscdigital.com/category/i...
Register for LSC Digital's next FREE webinar here: https://www.lscdigital.com/lsc-digita...
Subscribe to LSC Digital's YouTube channel for new videos every week: https://www.youtube.com/channel/UCywm...
Follow LSC Digital on Twitter: https://twitter.com/LSC_Digital
Follow LSC Digital on LinkedIn: https://www.linkedin.com/company/lscd...
